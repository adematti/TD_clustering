{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2f777c",
   "metadata": {},
   "source": [
    "## Cosmological constraints with BAO\n",
    "I provide you with a series of power spectrum measurements, at effective redshifts $z_{\\mathrm{zeff}}$ = 0.1 to 1.9.\n",
    "In this session, you will:\n",
    "\n",
    "1) measure the BAO isotropic parameter $\\alpha(z_{\\mathrm{eff}}) = \\left[D_{V}(z_{\\mathrm{eff}})/r_{\\mathrm{drag}}\\right] / \\left[D_{V}^{\\mathrm{fid}}(z_{\\mathrm{eff}})/r_{\\mathrm{drag}}^{\\mathrm{fid}}\\right]$ of each of the measurements\n",
    "\n",
    "2) with these measurements, you will constrain $\\Omega_{m}, \\Omega_{k}$ (hence detect dark energy!)\n",
    "\n",
    "3) provided a value of $r_{\\mathrm{drag}}$ (in $\\mathrm{Mpc}$), you will perform an \"inverse distance ladder\" analysis: fit all the $\\alpha(z_{\\mathrm{eff}})$ by varying $H_{0}$ and $\\Omega_{m}$\n",
    "\n",
    "4) those finding the correct $\\Omega_{m}$, $\\Omega_{k}$ and $H_{0}$ (I know the truth, you don't!) win!\n",
    "\n",
    "5) do you remember how to compute the correlation function? (last year's TD) What about the power spectrum? If any question, please ask!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26c221ac",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bc3e59d",
   "metadata": {},
   "source": [
    "#### Packages\n",
    "If you have already installed the packages, skip this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8bc6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install matplotlib cython emcee corner\n",
    "!python -m pip install git+https://github.com/cosmodesi/cosmoprimo#egg=cosmoprimo[class,astropy]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddea428e",
   "metadata": {},
   "source": [
    "#### Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/adematti/TD_clustering.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b437def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'TD_clustering')  # add to PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92de99",
   "metadata": {},
   "source": [
    "### A look at power spectrum measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfeea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from environment import Measurement, list_path_sim_measurement  # specify paths\n",
    "\n",
    "list_path = list_path_sim_measurement()\n",
    "list_data = []\n",
    "\n",
    "for path in list_path:\n",
    "    m = Measurement.load(path)\n",
    "    print('At effective redshift {:.2f} first k are {}, pk0 {}'.format(m.attrs['zeff'], m.x[:2], m.y[0,:2]))\n",
    "    plt.plot(m.x, m.x * m.y[0], label='$z = {:.2f}$'.format(m.attrs['zeff']))\n",
    "    list_data.append(m)\n",
    "\n",
    "plt.xlabel('$k$ [$h/\\mathrm{Mpc}$]')\n",
    "plt.ylabel('$k P(k)$ [$(\\mathrm{Mpc}/h)^{2}$]')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# m is an instance of Measurement\n",
    "# k (wavenumber) array is m.x\n",
    "# pk (power spectrum) is m.y\n",
    "# m.y is of shape (1, len(m.x)): the first Legendre multipole (monopole).\n",
    "print('Attributes: {}'.format(m.attrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de719ee",
   "metadata": {},
   "source": [
    "### Isotropic BAO power spectrum model\n",
    "\n",
    "The principle of BAO fits is to measure *only* the position of the BAO, marginalizing over the shape of the power spectrum, for this measurement to be robust to distortions due to observational systematics.\n",
    "For this we decompose the fiducial power spectrum into *wiggle* and *no-wiggle* parts. We will allow the *wiggle* part to move, keeping the *no-wiggle* part fixed, adding a polynomial in $k$ (*broadband terms*) to adjust the shape of the data power spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First generate a linear power spectrum in $(\\mathrm{Mpc}/h)^{3}$ units; z = 0 is enough.\n",
    "# (Indeed, linear growth factor D^{2}(z) is degenerate with B^2 in the model below)\n",
    "# Fiducial cosmological parameters are:\n",
    "# dict(Omega_m=0.31, omega_b=0.022, h=0.676, sigma8=0.8, n_s=0.97)\n",
    "# Several options:\n",
    "# 1) use cosmoprimo (engine='class' or engine='camb'), see https://github.com/cosmodesi/cosmoprimo/blob/main/nb/examples.ipynb\n",
    "#from cosmoprimo import Cosmology\n",
    "#cosmo_fid = Cosmology(Omega_m=0.31, omega_b=0.022, h=0.676, sigma8=0.8, n_s=0.97, engine='class')\n",
    "#klin = np.geomspace(1e-4, 5., 2000)\n",
    "#pk_interpolator = cosmo_fid.get_fourier().pk_interpolator().to_1d(z=0.)\n",
    "#pklin = pk_interpolator(klin)\n",
    "#np.savetxt('pklin.txt', np.column_stack([klin, pklin]))\n",
    "# 2) use classy, camb...\n",
    "# 3) load precomputed linear power spectrum:\n",
    "#klin, pklin = np.loadtxt('pklin.txt', unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a9ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you need a smooth power spectrum, matching the linear power spectrum but without BAO wiggles.\n",
    "# Several options:\n",
    "# 1) use cosmoprimo\n",
    "#from cosmoprimo import PowerSpectrumInterpolator1D\n",
    "#pk_interpolator = PowerSpectrumInterpolator1D(klin, pklin, extrap_kmin=1e-4, extrap_kmax=1e2)\n",
    "#from cosmoprimo import PowerSpectrumBAOFilter\n",
    "#pknow = PowerSpectrumBAOFilter(pk_interpolator, engine='wallish2018').smooth_pk_interpolator()(klin)\n",
    "# 2) fit the linear power spectrum in log/log with a polynomial (Hinton2017)\n",
    "#logk = np.log(klin)\n",
    "#logpk = np.log(pklin)\n",
    "#maxk = logk[pklin.argmax()]\n",
    "#gauss = np.exp(-0.5 * ((logk - maxk) / 1.)**2)\n",
    "#w = np.ones_like(klin) - 0.5 * gauss\n",
    "#series = np.polynomial.polynomial.Polynomial.fit(logk, logpk, deg=13 ,w=w)\n",
    "#pknow = np.exp(series(logk))\n",
    "# np.polynomial.polynomial.Polynomial.fit(logk, logpk, degree=13, w=w)\n",
    "# Plot wiggles = linear power spectrum / no-wiggle power spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot wiggles = linear power spectrum / no-wiggle power spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d01e4d",
   "metadata": {},
   "source": [
    "Model equation is (https://arxiv.org/abs/1705.06373, but many other formulations exist):\n",
    "$P(k,\\alpha) = B^{2} [P_{\\mathrm{nw}}(k) + \\sum_{i=0}^{2} A_{i} k^{i}] \\lbrace 1 + [\\mathcal{O}(k/\\alpha) - 1] e^{- \\frac{1}{2} \\Sigma_{\\mathrm{nl}}^{2}k^{2}} \\rbrace$  \n",
    "$P_{\\mathrm{nw}}$ is the no-wiggle power spectrum.  \n",
    "$\\mathcal{O}(k) = P_{\\mathrm{lin}}(k)/P_{\\mathrm{nw}}(k)$ are the wiggles.  \n",
    "Free parameters are: $\\alpha, B, A_{i}$, parameter of cosmological interest is $\\alpha$ (which shifts BAO wiggles).\n",
    "Choose for the non-linear damping of BAO wiggles $\\Sigma_{\\mathrm{nl}} = 6.7\\;\\mathrm{Mpc/h}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54290a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmanl = 6.7\n",
    "\n",
    "def pk_iso(k, alpha, B, A0, A1, A2, A3=0):\n",
    "    \"\"\"Code here BAO power spectrum model\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bcb661",
   "metadata": {},
   "source": [
    "### Model fitting\n",
    "Repeat, for each redshift z:  \n",
    "- build $\\chi^{2} = (\\mathrm{data} - \\mathrm{model})^T C^{-1} (\\mathrm{data} - \\mathrm{model})$\n",
    "- minimize chi2 for example with scipy.optimize.minimize, and check the fit looks good by plotting data and best fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "nbb = 3\n",
    "init = [1., 2.] + [0.]*nbb  # alpha, B, broadbands\n",
    "bounds = [(0.88, 1.12), (1., 5.)] + [(-5e4, 5e4)]*nbb\n",
    "\n",
    "for m in list_data:\n",
    "    covariance = m.cov\n",
    "    invcovariance = np.linalg.inv(covariance)\n",
    "    k = m.x\n",
    "    data = m.y[0]\n",
    "    \n",
    "    def chi2(args):\n",
    "        \"\"\"Write chi2\"\"\"\n",
    "    \n",
    "    #result = optimize.minimize(chi2, init, method='SLSQP', bounds=bounds)\n",
    "    #args = result.x\n",
    "    \"\"\"Check reduced chi2, make plot of data, model, error bars\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfbe0d4",
   "metadata": {},
   "source": [
    "We want an error on our alpha measurement. Either:\n",
    "- likelihood profiling: repeat the above for alpha values in e.g. [0.9, 1.1], and take errors at $\\chi_{\\mathrm{min}}^{2} + 1$\n",
    "- likelihood sampling: use sampler, e.g. https://github.com/dfm/emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56979395",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import emcee\n",
    "#import corner\n",
    "\n",
    "nsteps = 4000\n",
    "ndim = len(bounds)\n",
    "nwalkers = 2 * ndim\n",
    "list_alpha_mean, list_alpha_covariance = [], []\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "for m in list_data:\n",
    "\n",
    "    covariance = m.cov\n",
    "    invcovariance = np.linalg.inv(covariance)\n",
    "    k = m.x\n",
    "    data = m.y[0]\n",
    "    \n",
    "    def chi2(args):\n",
    "        \"\"\"Write chi2\"\"\"\n",
    "    \n",
    "    def logprior(args):\n",
    "        \"\"\"Write flat prior between bounds\"\"\"\n",
    "    \n",
    "    # Write posterior here (take flat priors between bounds on all parameters)\n",
    "    def logposterior(args):\n",
    "        \"\"\"Write log-posterior\"\"\"\n",
    "    \n",
    "    #sampler = emcee.EnsembleSampler(nwalkers, ndim, logposterior)\n",
    "    #start = [[rng.normal(v, (b[1] - b[0]) / 10.) for v, b in zip(init, bounds)] for i in range(nwalkers)]\n",
    "    #sampler.run_mcmc(start, nsteps)\n",
    "    #samples = sampler.get_chain(flat=True)\n",
    "    # Look at samples, remove burnin\n",
    "    #samples = samples[len(samples) * 3 // 4:]\n",
    "    # Take mean (or median) and covariance of samples\n",
    "    #list_alpha_mean.append(mean)\n",
    "    #list_alpha_covariance.append(covariance)\n",
    "    #labels = ['alpha','B'] + ['A{:d}'.format(i) for i in range(nbb)]\n",
    "    #fig = corner.corner(samples, labels=labels, quantiles=[0.16, 0.5, 0.84], show_titles=True, title_kwargs={'fontsize': 14})\n",
    "    #plt.show()\n",
    "    #plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ecfc9",
   "metadata": {},
   "source": [
    "### Expansion history\n",
    "The measured $\\alpha$ can be modelled by $\\alpha_{\\mathrm{theory}}(z) = \\left[D_{V}(z_{\\mathrm{eff}})/r_{\\mathrm{drag}}\\right] / \\left[D_{V}^{\\mathrm{fid}}(z_{\\mathrm{eff}})/r_{\\mathrm{drag}}^{\\mathrm{fid}}\\right]$ (why?)  \n",
    "$D_{V}(z) = z^{1/3} D_{M}(z)^{2/3} D_{H}(z)^{1/3}$ with $D_{M}(z)$ comoving transverse distance, $D_{H}(z) = c/H(z)$ \"Hubble distance\".\n",
    "\n",
    "Code up $D_{M}(z)$, $D_{H}(z)$ as a function of $\\Omega_{m}$ and $\\Omega_{k}$ (faster than creating a new cosmology at each MCMC step).  \n",
    "Build the $\\chi^{2}$ of the previously measured $\\alpha$, using $\\alpha_{\\mathrm{theory}}$ as a model.\n",
    "Previously measured $\\alpha$ `list_alpha_mean` should be considered independent (zero cross-correlations); hence the covariance matrix is diagonal, with `list_alpha_covariance` as diagonal elements.\n",
    "This $\\chi^{2}$ depends on $r_{\\mathrm{drag}}$ (in $\\mathrm{Mpc}/h$), $\\Omega_{m}$ and $\\Omega_{k}$ if free curvature (entering $D_{M}$ and $D_{H}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18dd95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_z, list_dv_over_rs_fid = [], []\n",
    "for data in list_data:\n",
    "    z = data.attrs['zeff']\n",
    "    #list_z.append(z)\n",
    "    # Precompute DV_fid / rs_drag_fid\n",
    "    # dv = z**(1. / 3.) * ((1 + z) * cosmo_fid.angular_diameter_distance(z))**(2. / 3.) * (100. * cosmo_fid.efunc(z))**(-1. / 3.) /\n",
    "    #list_dv_over_rs_fid.append(dv)\n",
    "\n",
    "list_z = np.array(list_z)\n",
    "list_dv_over_rs_fid = np.array(list_dv_over_rs_fid)\n",
    "\n",
    "# Define Hubble parameter, comoving angular distance, spherically-averaged distance DV\n",
    "# model, function of (rs_drag, Omega_m, Omega_k), is DV/rs_drag / (DV_fid / rs_drag_fid) at each data z."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf1021d",
   "metadata": {},
   "source": [
    "Use a very wide prior on $r_{\\mathrm{drag}}$ as we don't want to make any assumption about the primordial Universe: we only assume constant comoving $r_{\\mathrm{drag}}$ (\"standard ruler\") to constrain expansion history.\n",
    "Sample the posterior and draw the contours $\\Omega_{m} - \\Omega_{k}$. See? we detect dark energy! (and zero curvature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c07a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2(args):\n",
    "    \"\"\"Write chi2, assuming independent measurements at each redshift.\"\"\"\n",
    "\n",
    "# For rdrag (Mpc/h), Omega_m, Omega_k\n",
    "init = [100., 0.3, 0.]  # rs_drag, Omega_m, Omega_k \n",
    "bounds = [(20, 400), (0.01, 0.9), (-0.8, 0.8)]\n",
    "\n",
    "def logprior(args):\n",
    "    \"\"\"Write flat prior between bounds\"\"\"\n",
    "\n",
    "def logposterior(args):\n",
    "    \"\"\"Write log-posterior\"\"\"\n",
    "\n",
    "#ndim = len(init)\n",
    "#nwalkers = 4 * ndim\n",
    "#nsteps = 2000\n",
    "#sampler = emcee.EnsembleSampler(nwalkers, ndim, logposterior)\n",
    "#start = [[rng.normal(v, (b[1] - b[0]) / 40.) for v, b in zip(init, bounds)] for i in range(nwalkers)]\n",
    "#sampler.run_mcmc(start, nsteps)\n",
    "#samples = sampler.get_chain(flat=True)\n",
    "# Remove burnin and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a315d",
   "metadata": {},
   "source": [
    "### Inverse distance ladder\n",
    "Do the same, but for $r_{\\mathrm{drag}}$ use a Gaussian prior $r_{\\mathrm{drag}} = 149.47 \\pm 0.48 \\; \\mathrm{Mpc}$.  \n",
    "Assume zero curvature for this exercise (though you can let it free in general, in addition to different dark energy models --- all those are constrained by BAO).\n",
    "Sample the posterior and draw the $\\Omega_{m} - H_{0}$ contours. See? given $r_{\\mathrm{drag}}$ we constrain $H_{0}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebe843",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = [149.47, 70, 0.3]  # rs_drag, H0, Omega_m \n",
    "bounds = [(100, 200), (50, 100), (0.1, 0.5)]\n",
    "\n",
    "#ndim = len(init)\n",
    "#nwalkers = 4 * ndim\n",
    "#nsteps = 3000\n",
    "#sampler = emcee.EnsembleSampler(nwalkers, ndim, logposterior)\n",
    "#start = [[rng.normal(v, (b[1] - b[0]) / 10.) for v, b in zip(init, bounds)] for i in range(nwalkers)]\n",
    "#sampler.run_mcmc(start, nsteps)\n",
    "#samples = sampler.get_chain(flat=True)\n",
    "# Remove burnin and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4af19",
   "metadata": {},
   "source": [
    "### BBN prior\n",
    "CMB constraints provide a measurement of $r_{\\mathrm{drag}}$ (through $\\omega_{cdm}$ and $\\omega_{b}$),\n",
    "but we only need a prior on $\\omega_{b} = \\Omega_{b} h^{2}$ since $\\Omega_{m}$ is constrained by BAO mesurements.  $\\omega_{b} = \\Omega_{b} h^{2}$ is constrained by measurements of deuterium abundance as a result of BBN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute rs_drag, either:\n",
    "# 1) use cosmoprimo (engine='class')\n",
    "#def rs_drag(omega_b, omega_m):\n",
    "#    return cosmo_fid.clone(omega_b=omega_b, omega_m=omega_m, engine='class').rs_drag / cosmo_fid.h\n",
    "# 2) use cosmoprimo (engine='eisentein_hu')\n",
    "#def rs_drag(omega_b, omega_m):\n",
    "#    return cosmo_fid.clone(omega_b=omega_b, omega_m=omega_m, engine='eisenstein_hu').rs_drag / cosmo_fid.h\n",
    "# 3) implement rs_drag yourself, following https://arxiv.org/abs/astro-ph/9709112, eq. 4 - 6.\n",
    "# For $z_{\\mathrm{drag}}$, rather use https://arxiv.org/abs/astro-ph/9510117, eq. E1 (found better match to Boltzmann code).\n",
    "# Fix $T_\\mathrm{cmb} = 2.7255$ (Cobe-FIRAS).\n",
    "#def rs_drag(omega_b, omega_m):\n",
    "#    \"\"\"Write approximation for rs_drag as a function of omega_b, omega_m\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d4e51",
   "metadata": {},
   "source": [
    "Redo the previous sampling, varying $H_{0}$, $\\Omega_{m}$ and $\\omega_{b}$ with the BBN prior: $\\omega_{b} = 0.02235 \\pm 0.00037$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For omega_b, H0, Omega_m\n",
    "init = [0.02235, 70, 0.3]  # omega_b, H0, Omega_m\n",
    "bounds = [(0.01, 0.03), (50, 100), (0.1, 0.5)]\n",
    "\n",
    "\n",
    "#ndim = len(init)\n",
    "#nwalkers = 4 * ndim\n",
    "#nsteps = 3000\n",
    "#sampler = emcee.EnsembleSampler(nwalkers, ndim, logposterior)\n",
    "#start = [[rng.normal(v, (b[1] - b[0]) / 10.) for v, b in zip(init, bounds)] for i in range(nwalkers)]\n",
    "#sampler.run_mcmc(start, nsteps)\n",
    "#samples = sampler.get_chain(flat=True)\n",
    "# Remove burnin and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493f13a",
   "metadata": {},
   "source": [
    "## Take-home messages\n",
    "- BAO feature is a standard ruler: measuring the position of the BAO peak = measuring a fixed comoving distance (sound horizon at the drag epoch) at a given redshift = constraing the Universe's expansion\n",
    "- assuming standard ruler only (of unspecified length), we can constrain the 'derivatives of the expansion', i.e. the matter / dark energy density\n",
    "- with an additional prior on $r_{\\mathrm{drag}}$ (or $\\omega_{b}$), we can measure $H_{0}$ = 'inverse distance ladder'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2bbdf",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "- see how $\\Omega_{m} - H_{0}$ contour rotates with redshift\n",
    "- perform fits on real eBOSS data (see eBOSS_LRGpCMASS.ipynb for power spectrum measurements): isotropic, anisotropic BAO fits, RSD fits, ...\n",
    "- why are you still here? go to the beach..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmodesi",
   "language": "python",
   "name": "cosmodesi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
