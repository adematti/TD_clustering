{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b7425e",
   "metadata": {},
   "source": [
    "## Hands on real clustering data - eBOSS LRG (plus BOSS CMASS) sample\n",
    "In this session we will estimate (and interpret) the correlation function / power spectrum of galaxy catalogs.\n",
    "This is the first step of a standard clustering analysis; the second step consists in fitting these compressed measurements with a theory model to extract cosmological parameters, which will be explored in the Y2 TD (bao_inverse_distance_ladder.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd0667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from astropy.table import Table\n",
    "\n",
    "import environment\n",
    "from environment import Measurement  # specify paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f786d2b",
   "metadata": {},
   "source": [
    "## Inspecting catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d735d0a2",
   "metadata": {},
   "source": [
    "Download eBOSS LRGpCMASS catalogs here: https://drive.google.com/drive/folders/1bs0YSEwCOghx2YzCb2IbdIkn-7uttQz-?usp=sharing\n",
    "  \n",
    "(These are a \"light version\" of the official catalogs provided at https://data.sdss.org/sas/dr16/eboss/lss/catalogs/DR16/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25c0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer = 'LRGpCMASS'\n",
    "cap = 'NGC'\n",
    "recon = False\n",
    "# base_dir is the directory where you saved the catalogs\n",
    "path_data, path_randoms = environment.path_catalogs(tracer=tracer, cap=cap, recon=recon, base_dir='./catalogs/lite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f471a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TableColumns names=('RA','DEC','Z','WEIGHT_SYSTOT','WEIGHT_CP','WEIGHT_NOZ','NZ','WEIGHT_FKP')>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Table.read(path_data)\n",
    "randoms = Table.read(path_randoms)\n",
    "data.columns\n",
    "randoms.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aca63d",
   "metadata": {},
   "source": [
    "Galaxies (and randoms) in the catalog receive weights, to correct for observational systematic effects, such that the ensemble average of galaxy density (= \"survey selection function\") and that of randoms match:\n",
    "- WEIGHT_SYSTOT: weights to correct for photometric systematics: what are they?\n",
    "- WEIGHT_CP: weights to correct for fiber collisions: what are they?\n",
    "- WEIGHT_NOZ: weights to correct for redshift failures: what are they?\n",
    "\n",
    "The total (completenes) weight is: WEIGHT_COMP = WEIGHT_SYSTOT * WEIGHT_CP * WEIGHT_NOZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c84aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot of RA/Dec, histogram as a function of redshift\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6790359",
   "metadata": {},
   "source": [
    "To compute the 3D correlation function or power spectrum, we first need to transform redshifts Z into distances, assuming a fiducial cosmology. Let's take BOSS (and eBOSS) fiducial cosmology:\n",
    "$\\Omega_{m} = 0.31$, $\\omega_{b} = 0.022$, $h = 0.676$, $\\sigma_{8} = 0.8$, $n_{s} = 0.97$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfd49e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiducial cosmological parameters are:\n",
    "# dict(Omega_m=0.31, omega_b=0.022, h=0.676, sigma8=0.8, n_s=0.97)\n",
    "# To compute comoving_radial_distance(z), various options:\n",
    "# 1) code this yourself if you have never done so!\n",
    "# H(z) = H_{0} \\sqrt{\\Omega_{m} (1 + z)^3 + \\Omega_{\\Lambda}}\n",
    "# d(z) = \\int cdz / H(z)\n",
    "# WARNING: let's work in Mpc/h units (H0 = 100 h km/s/Mpc)\n",
    "# 2) use classy, camb, astropy...\n",
    "# 3) use cosmoprimo (engine='class' or 'camb' or 'astropy'), see cell 9 of https://github.com/cosmodesi/cosmoprimo/blob/main/nb/examples.ipynb\n",
    "#from cosmoprimo import Cosmology\n",
    "#cosmo_fid = Cosmology(Omega_m=0.31, omega_b=0.022, h=0.676, sigma8=0.8, n_s=0.97, engine='class')\n",
    "# Or simply:\n",
    "#from cosmoprimo.fiducial import BOSS\n",
    "#cosmo_fid = BOSS(engine='class')\n",
    "# Compute vector of data positions (RA, DEC, d(Z))\n",
    "#data_positions =\n",
    "# Same for randoms\n",
    "#randoms_positions =\n",
    "# You can also turn this to x, y, z, and plot the 3D distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166eaf32",
   "metadata": {},
   "source": [
    "In addition to completeness weights above, you can apply weights to minimize variance: WEIGHT_FKP = 1/(1 + NZ * P0), with P0 the typical value of the power spectrum at the scales of interest, e.g. $10000 \\; (\\mathrm{Mpc}/h)^{3}$ (NZ is in $(\\mathrm{Mpc}/h)^{3}$).\n",
    "See e.g. https://arxiv.org/pdf/astro-ph/9304022.pdf, eq. 2.3, for the variational demonstration (another, broader point-of-view is that of the optimal quadratic estimator, of which the FKP estimator we will use below is a simplification under some assumptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e257e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute data weights, WEIGHT_COMP * WEIGHT_FKP\n",
    "#data_weights = \n",
    "# Same for randoms\n",
    "#randoms_weights = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c8b2f5",
   "metadata": {},
   "source": [
    "## Correlation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d76792",
   "metadata": {},
   "source": [
    "### Pair counts as a function of $(s, \\mu)$\n",
    "\n",
    "Correlation functions are typically estimated binning the (weighted) number of pairs of particles (galaxies, randoms) as a function of the separation $s$ between particles (and optionally the cosine angle to the line-of-sight $\\mu$).\n",
    "\n",
    "This is basically a double loop, schematically:\n",
    "\n",
    "for i1 in n1:  \n",
    "    for i2 in n2:  \n",
    "        counts[index(s(i1, i2)), index(mu(i1, i2))] += weight(i1, i2)  \n",
    "\n",
    "For n1 ~ n2 ~ O(1e6) objects, this will be slow (especially in Python). How to speed this up?\n",
    "Auto pair counts can be compute ~ twice faster than cross pair counts, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a898a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute galaxy - galaxy pair counts DD\n",
    "# You can use pycorr (wrapping a modified version of Corrfunc)\n",
    "# If so, take a quick look at https://github.com/cosmodesi/pycorr/blob/main/nb/basic_examples.ipynb\n",
    "#edges = (np.linspace(0., 200, 201), np.linspace(-1., 1., 100))  # s, mu binning\n",
    "#from pycorr import TwoPointCounter\n",
    "# If positions are x, y, z, pass position_type='xyz' instead\n",
    "#D1D2 = TwoPointCounter(mode='smu', edges=edges, positions1=data_positions, weights1=data_weights,\n",
    "#                       position_type='rdd', nthreads=4, dtype='f8', engine='corrfunc')\n",
    "# D1D2.sep are (average) separations\n",
    "# D1D2.wcounts are pair counts (weighted by the product of particle weights)\n",
    "# D1D2.wnorm is the normalization (see below)\n",
    "# Same for randoms - randoms pair counts RR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59a910",
   "metadata": {},
   "source": [
    "Can you guess how RR counts typically evolve as a function of $s$ at small separation? This relation is not perfect at small scales, due to fine-grained veto masks, but why is it really not valid anymore at large scales? You see now why randoms are important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c1eabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RR / f(s) counts as a function of s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3a342",
   "metadata": {},
   "source": [
    "### Natural estimator\n",
    "The natural estimator (the simplest one can typically think of) for the correlation function compares the (weighted) number of galaxy pairs to that of randoms (what we would have in absence of clustering), i.e. DD / RR - 1, with:\n",
    "- DD the (normalized) galaxy - galaxy pair counts\n",
    "- RR the (normalized) randoms - randoms pair counts\n",
    "Why 'normalized'? What is the normalization factor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe9486b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DD / RR - 1\n",
    "# WARNING: you need to normalize DD and RR first! By what factor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3aff69",
   "metadata": {},
   "source": [
    "### Multipoles of the correlation function\n",
    "Compute and plot the correlation function multipoles $\\ell = 0$ (monopole), $\\ell = 2$ (quadrupole) and $\\ell = 4$ (hexadecapole), given by $\\xi_{\\ell}(s) = \\frac{2 \\ell + 1}{2} \\int_{-1}^{1} d\\mu \\xi(s,\\mu) \\mathcal{L}_{\\ell}(\\mu)$,\n",
    "with $\\mathcal{L}_{\\ell}(\\mu)$ Legendre polynomials (see https://en.wikipedia.org/wiki/Legendre_polynomials, also https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.legendre.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76ce3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick: write d\\mu \\mathcal{L}_{\\ell}(\\mu) as the difference of \\mathcal{L}_{\\ell}(\\mu) primitives\n",
    "ells = (0, 2, 4)\n",
    "# Compute xiell, list of multipoles\n",
    "# Compute sep, mean s\n",
    "\n",
    "# Plot the correlation function multipoles\n",
    "#for ill, ell in enumerate(ells):\n",
    "#    plt.plot(sep, sep**2 * xiell[ill], label='$\\ell = {:d}$'.format(ell))\n",
    "#plt.xlabel(r'$s$ [$\\mathrm{Mpc}/h$]')\n",
    "#plt.ylabel(r'$s^{2}\\xi_{\\ell}(s)$ [$(\\mathrm{Mpc}/h)^{2}$]')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da18beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes about pycorr\n",
    "# You can obtain this directly with:\n",
    "#from pycorr import NaturalTwoPointEstimator\n",
    "#estimator = NaturalTwoPointEstimator(D1D2=D1D2, R1R2=R1R2)\n",
    "#s, xiell = estimator(ells=ells, return_sep=True)\n",
    "#estimator.plot(ells=ells)\n",
    "# Or, starting from the beginning:\n",
    "#from pycorr import TwoPointCorrelationFunction\n",
    "#estimator = TwoPointCorrelationFunction(mode='smu', edges=edges, data_positions1=data_positions, data_weights1=data_weights,\n",
    "#                                        randoms_positions1=randoms_positions, randoms_weights1=randoms_weights,\n",
    "#                                        estimator='natural', position_type='rdd', nthreads=4, dtype='f8', engine='corrfunc')\n",
    "#estimator.plot(ells=ells)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc33a4",
   "metadata": {},
   "source": [
    "The monopole looks odd on large scales! No worries, this is not bias (in average, it would be fine), but the natural estimator has a larger variance than the Landy-Szalay estimator (see e.g. https://articles.adsabs.harvard.edu/pdf/1993ApJ...412...64L): (DD - 2DR + RR) / RR.\n",
    "Estimate the correlation function with the Landy-Szalay estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2450f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can compute all pair counts DD, DR and RR as above\n",
    "# Or use pycorr's general interface (see https://github.com/cosmodesi/pycorr/blob/main/nb/basic_examples.ipynb)\n",
    "#from pycorr import TwoPointCorrelationFunction\n",
    "#estimator = TwoPointCorrelationFunction(mode='smu', edges=edges, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd42806",
   "metadata": {},
   "source": [
    "Plot the correlation function multipoles. Looks better!  \n",
    "You should clearly see the BAO peak (where?) You can compare to a theory correlation function, scaled by hand to match the amplitude of the data correlation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8605db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "#estimator.plot(ells=ells)\n",
    "#zeff = np.average(data['Z'], weights=data_weights)  # (quite arbitrary) effective redshift\n",
    "#kaiser_factor =   # scaling by hand, just to match the amplitude\n",
    "#xi_model = kaiser_factor * cosmo_fid.get_fourier().pk_interpolator().to_xi()(sep, z=zeff)\n",
    "#plt.plot(sep, sep**2 * xi_model, color='k')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f8095",
   "metadata": {},
   "source": [
    "What does the non-zero quadrupole mean? Where does it come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12371e26",
   "metadata": {},
   "source": [
    "## Power spectrum\n",
    "\n",
    "Computing pair counts for correlation function estimation remains somewhat slow (still tractable for current surveys, DESI, Euclid). Also, theoreticians tend to prefer the power spectrum, as k-modes evolve independently in the linear regime (see Julien's course).\n",
    "Let's compute the power spectrum monopole step-by-step, to show how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c393454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypower import CatalogMesh\n",
    "# Paint catalogs to mesh\n",
    "# This involves a kernel (resampler='tsc') that effectively smoothes the density field,\n",
    "# which must be compensated for by a kernel in Fourier space (compensate=True),\n",
    "# see e.g. https://arxiv.org/abs/astro-ph/0409240\n",
    "# Wider is this kernel, better is the mitigation of aliasing effects (greater than Nyquist frequencies contaminating lower frequencies)\n",
    "# Another technique to mitigate aliasing is 'interlacing': shifting the mesh by a fraction of mesh cell size\n",
    "# see e.g. https://arxiv.org/abs/1512.07295\n",
    "#mesh = CatalogMesh(data_positions=data_positions, data_weights=data_weights,\n",
    "#                     randoms_positions=randoms_positions, randoms_weights=randoms_weights,\n",
    "#                     boxsize=5000., nmesh=256, resampler='tsc', interlacing=3, position_type='rdd',\n",
    "#                     dtype='f8', mpiroot=0)\n",
    "#rfield = mesh.to_mesh(compensate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "348f48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The power spectrum is (up to some normalization), the square modulus of the Fourier-space field: P(k) ~ |F(k)|^2\n",
    "#cfield = rfield.r2c()  # Fast Fourier Transform, F(r) -> F(k)\n",
    "#cfield[...] = cfield[...] * cfield[...].conj()\n",
    "#del rfield  # save memory\n",
    "#print(cfield.value[0, 0, 0])  0 by construction: integral constraint!\n",
    "\n",
    "# Now, bin as a function of |k|\n",
    "# edges = (k-edges, mu-edges)\n",
    "#knyq = np.pi * np.min(mesh.nmesh / mesh.boxsize)  # Nyquist frequency\n",
    "#edges = (np.arange(0., knyq, 0.005), np.linspace(-1., 1., 2))\n",
    "#from pypower.fft_power import project_to_basis\n",
    "#k, _, pk, nk, _ = project_to_basis(cfield, edges)[0]\n",
    "#k, pk = k.ravel(), pk.ravel().real\n",
    "#del cfield  # save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a2fe8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the normalization\n",
    "# FFT convention is F(k) = 1/N^3 \\sum_{r} e^{-ikr} F(r), so compensate by N^6\n",
    "#pk *= mesh.nmesh.prod()**2\n",
    "# Then, we will typically normalize by something close to A = 1/dV \\sum_{r} \\bar{F}^2(r)\n",
    "# Trick: data * randoms instead of randoms**2 to avoid shot noise\n",
    "#wnorm = (mesh.to_mesh(field='data', compensate=False) * mesh.to_mesh(field='data-normalized_randoms', compensate=False)).csum() / np.prod(mesh.boxsize / mesh.nmesh)\n",
    "#pk /= wnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb45a2d",
   "metadata": {},
   "source": [
    "Plot the power spectrum (typically $k P(k)$). What is its unit?\n",
    "The small scales (high $k$) monopole looks fishy (too high), did we forget anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a2b6ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a67484",
   "metadata": {},
   "source": [
    "Indeed, we forgot to remove the Poisson shot noise! What is it?\n",
    "It is computed as $1/A (\\sum_{i \\in \\mathrm{data}} w_{i}^{2} + \\alpha^2 \\sum_{i \\in \\mathrm{randoms}} w_{i}^{2}$, with $\\alpha = \\sum_{i \\in \\mathrm{data}} w_{i} / \\sum_{i \\in \\mathrm{randoms}} w_{i})$.\n",
    "Subtract it from the esimated power spectrum, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "894c1c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd9367",
   "metadata": {},
   "source": [
    "Looks much better! Do you recognize the BAO wiggles? To see them better, you can compare data to a theory power spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2b90f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "#pk_model = kaiser_factor * cosmo_fid.get_fourier().pk_interpolator()(k, z=zeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae03bc",
   "metadata": {},
   "source": [
    "### Multipoles of the power spectrum\n",
    "Estimating higher multipoles (quadrupole, hexadecapole) is slightly more difficult, as we have to account for varying line-of-sight. We typically use the first-point / end-point for line-of-sight $\\eta$, to split:\n",
    "$\\hat{P}(k) = 1 / A \\sum_{r_{1}, r_{2}} e^{ik(r_{2} - r_{1})} F(r_{1}) F(r_{2}) \\mathcal{L}_{\\ell}(k \\cdot \\eta)$\n",
    "into:\n",
    "$\\hat{P}(k) = 1 / A F(k)^{\\star} F_{\\ell}(k)$\n",
    "with:\n",
    "$F_{\\ell}(k) = \\sum_{r} e^{ikr} F(r) \\mathcal{L}_{\\ell}(k \\cdot r)$\n",
    "This last term can be split, using e.g. decomposition of Legendre polynomial into spherical harmonics, into a pure k-dependent term, times a standard Fourier transform, which can be FFTed. See e.g. https://arxiv.org/abs/1704.02357."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "365a2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this, use pypower.\n",
    "# (You can take a quick look at https://github.com/cosmodesi/pypower/blob/main/nb/basic_examples.ipynb)\n",
    "#from pypower import CatalogFFTPower\n",
    "#power = CatalogFFTPower(data_positions1=data_positions, data_weights1=data_weights,\n",
    "#                        randoms_positions1=randoms_positions, randoms_weights1=randoms_weights, ...).poles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f2a0b9",
   "metadata": {},
   "source": [
    "Plot the power spectrum multipoles.\n",
    "Again, BAO wiggles and non-zero quadrupole!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4e45df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "#power.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45265bc",
   "metadata": {},
   "source": [
    "Note that the above power spectrum measurements (using the 'FKP estimator') cannot (should not) be directly compared to theory power spectrum models... Indeed, in these measurements the power spectrum is convolved with the survey selection function (that we essentially already removed in the correlation function estimation with the division by RR). Therefore, usually people multiply the theory power spectrum by the window matrix, which can be computed from the random catalog, before comparing to the data; see https://github.com/cosmodesi/pypower/blob/main/nb/window_examples.ipynb if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b76a2",
   "metadata": {},
   "source": [
    "### Kaiser formula\n",
    "Still, let's just look how the power spectrum multipoles compare to the Kaiser formula, which gives the expected power spectrum in the linear approximation:\n",
    "$P(k, \\mu) = (b + f \\mu^2)^{2} P^{\\mathrm{lin}}(k) = (\\beta^{-1} + \\mu^2)^{2} P^{\\mathrm{lin}}_{\\theta\\theta}(k)$ ($\\beta = f / b$)\n",
    "where $b$ is the galaxy bias, $f = \\frac{d\\ln{D}}{d\\ln{a}} \\simeq \\Omega_m(z)^{0.55}$ is the logarithmic growth rate of structure, $P^{\\mathrm{lin}}(k)$ is the linear power spectrum.\n",
    "\n",
    "The model depends on $f (\\times \\sigma_{8})$ or --- better formulated --- the amplitude of the velocity divergence power spectrum $P_{\\theta\\theta}(k)$, which we can fit to the data.\n",
    "\n",
    "Integrate $P(k, \\mu)$ over Legendre polynomials, either symbolically (easy!) or by numerical integration, and plot model and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72945f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pklin = cosmo_fid.get_fourier().pk_interpolator()(k, z=zeff)\n",
    "#f = cosmo_fid.growth_rate(zeff)\n",
    "b = 2.25  # tune by hand to match data amplitude\n",
    "pk_model = []\n",
    "# Append the multipoles\n",
    "#power.plot()\n",
    "#for ill, ell in enumerate(ells):\n",
    "#    plt.plot(k, k * pk_model[ill], color='C{:d}'.format(ill), linestyle='--')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e12a7",
   "metadata": {},
   "source": [
    "The quadrupole in particular looks a small scales (high $k$) a bit more damped in the data than in the model, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397adbe",
   "metadata": {},
   "source": [
    "## Take-home messages\n",
    "- in standard analyses, the observed (RA, Dec, z) are compressed into power spectrum or correlation function multipoles\n",
    "- BAO: peak at 100 $\\mathrm{Mpc}/h$ in the correlation function, wiggles in the power spectrum\n",
    "- measuring the position of the BAO peak = measuring a fixed comoving distance ('standard ruler') at a given redshift = constraing the Universe's expansion\n",
    "- non-zero quadrupole = anisotropy (w.r.t the line-of-sight): redshift-space distortions (linear model: Kaiser formula)\n",
    "- measuring redshift-space distortions = constraining the amplitude of the velocity power spectrum: growth of structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5973ea6",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "- pen-and-paper exercise: derive the Kaiser formula, knowing that:\n",
    "$\\mathbf{s} = \\mathbf{r} + v_{z} \\hat{\\mathbf{z}}$ with $v_{z}(\\mathbf{k}) = i f \\mathbf{k} / k^{2} \\delta_{r}(\\mathbf{k})$ (which can be obtained from linear theory)\n",
    "and mass conservation (mass in redshift space = mass in real space): $\\left[1 + \\delta_{s}(\\mathbf{s})\\right]d^{3}s = \\left[1 + \\delta_{r}(\\mathbf{r})\\right]d^{3}r$\n",
    "- we have worked so far with pre-reconstruction catalogs. A step called 'reconstruction' can be used to sharpen the BAO peak. Compute the correlation function and / or power spectrum of reconstructed catalogs (with *rec* in name).\n",
    "Note that, in this case, you should provide reconstructed data for data_positions and data_weights, reconstructed randoms for shifted_positions and shifted_weights, and pre-reconstruction randoms for randoms_positions, randons_weights. What is the difference w.r.t. pre-reconstruction measurements? (BAO peak, quadrupole?).\n",
    "- you can try applying reconstruction yourself to pre-reconstruction catalogs with https://github.com/cosmodesi/pyrecon\n",
    "- here pair weights are simple product of galaxy individual weights... but we can imagine other schemes, e.g. to correctly compensate for fiber collisions: see https://github.com/cosmodesi/pycorr/blob/main/nb/pip_examples.ipynb (and similar correction for power spectrum, https://github.com/cosmodesi/pypower/blob/main/nb/pip_examples.ipynb)\n",
    "- start Y2 TD, cosmological constraints with BAO: bao_inverse_distance_ladder.ipynb\n",
    "- why are you still here? go to the beach..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmodesi",
   "language": "python",
   "name": "cosmodesi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
