{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b7425e",
   "metadata": {},
   "source": [
    "## Hands on real clustering data - eBOSS LRG (plus BOSS CMASS) sample\n",
    "In this session we will estimate (and interpret) the correlation function / power spectrum of galaxy catalogs.\n",
    "This is the first (compression) step of a standard clustering analysis; the second step consists in fitting these compressed measurements with a theory model to derive constraints on cosmological parameters, which will be explored in the Y2 TD (bao_inverse_distance_ladder.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e0cb97",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6a01e",
   "metadata": {},
   "source": [
    "#### Packages\n",
    "**If packages are already installed, skip this part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe856e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install matplotlib cython mpi4py fitsio\n",
    "!python -m pip install git+https://github.com/cosmodesi/cosmoprimo#egg=cosmoprimo[class,astropy]\n",
    "!USE_GPU=0 python -m pip install git+https://github.com/cosmodesi/pycorr#egg=pycorr[corrfunc]\n",
    "# When running on Google Colab, you can use GPU (setting \"execution type\" to GPU):\n",
    "#!CUDA_HOME=/usr/local/cuda python -m pip install git+https://github.com/cosmodesi/pycorr#egg=pycorr[corrfunc]\n",
    "!python -m pip install git+https://github.com/cosmodesi/pypower#egg=pypower[extras]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbbf668",
   "metadata": {},
   "source": [
    "#### Note\n",
    "Within the DESI collaboration we have put some effort into setting up some ~ easy to use Python packages for standard clustering analyses. Check them out here: https://github.com/cosmodesi.\n",
    "You may e.g. be interested by:\n",
    "- [cosmoprimo](https://github.com/cosmodesi/cosmoprimo): primordial cosmology (class, camb, isitgr, fftlog, interpolator, BAO filtering)\n",
    "- [pycorr](https://github.com/cosmodesi/pycorr): correlation function estimation\n",
    "- [pypower](https://github.com/cosmodesi/pypower): power spectrum (and window function) estimation\n",
    "- [pyrecon](https://github.com/cosmodesi/pyrecon): standard BAO reconstruction\n",
    "- [mockfactory](https://github.com/cosmodesi/mockfactory): tools to be build fast mocks\n",
    "- [desilike](https://github.com/cosmodesi/desilike): DESI likelihoods, fits of 2-pt statistics, Fisher, bindings with cosmological samplers (Cobaya, CosmoSIS, MontePython)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9950b91",
   "metadata": {},
   "source": [
    "#### Catalogs\n",
    "Let's download eBOSS LRGpCMASS catalogs here: https://drive.google.com/drive/folders/1bs0YSEwCOghx2YzCb2IbdIkn-7uttQz-?usp=sharing.\n",
    "These are a \"light version\" of the official catalogs provided at https://data.sdss.org/sas/dr16/eboss/lss/catalogs/DR16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1501cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install gdown\n",
    "!gdown --no-check-certificate --folder https://drive.google.com/drive/folders/1lN0xu7mWuu46POSaJ1t5tKCmtxOETfXh?usp=sharing\n",
    "#!gdown --no-check-certificate --folder https://drive.google.com/drive/folders/1egpLxKnteOQgYIetNDk7TFmk-vDx_O11?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f786d2b",
   "metadata": {},
   "source": [
    "## Inspecting catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './catalogs/lite/eBOSS_LRGpCMASS_clustering_data-NGC-vDR16_lite.fits'\n",
    "path_randoms = './catalogs/lite/eBOSS_LRGpCMASS_clustering_random-NGC-vDR16_lite.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f471a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from astropy.table import Table\n",
    "\n",
    "data = Table.read(path_data)\n",
    "randoms = Table.read(path_randoms)\n",
    "data.columns\n",
    "randoms.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aca63d",
   "metadata": {},
   "source": [
    "Galaxies in the data and randoms catalogs receive weights, to correct for observational systematic effects, such that the ensemble average of galaxy density (= \"survey selection function\") and that of randoms match:\n",
    "- WEIGHT_SYSTOT: weights to correct for photometric systematics: what are they?\n",
    "- WEIGHT_CP: weights to correct for fiber collisions: what are they?\n",
    "- WEIGHT_NOZ: weights to correct for redshift failures: what are they?\n",
    "\n",
    "The total (completeness) weight is: WEIGHT_COMP = WEIGHT_SYSTOT * WEIGHT_CP * WEIGHT_NOZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot of data RA/Dec and a histogram of data redshifts\n",
    "# Check that weighted randoms have the same angular (RA/Dec) and redshift (Z) distribution as the data\n",
    "from matplotlib import pyplot as plt\n",
    "# Tip: for the RA/Dec plot, downsample the data and randoms for faster plots:\n",
    "rng = np.random.RandomState(seed=42)\n",
    "mask_data = rng.uniform(0., 1., len(data)) < 0.1\n",
    "mask_randoms = rng.uniform(0., 1., len(randoms)) < mask_data.sum() / len(randoms)\n",
    "# Then, e.g. plt.scatter(data['RA'][mask_data], data['DEC'][mask_data], s=1, label='data')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for catalog in [data, randoms]:\n",
    "#    catalog['WEIGHT_COMP'] = define weights\n",
    "\n",
    "#Then, e.g.: plt.hist(data['Z'], weights=data['WEIGHT_COMP'], histtype='step', density=True, label='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6790359",
   "metadata": {},
   "source": [
    "Let's first transform redshifts Z into distances, assuming a fiducial cosmology. Take BOSS (and eBOSS) fiducial cosmology:\n",
    "$\\Omega_{m} = 0.31$, $\\omega_{b} = 0.022$, $h = 0.676$, $\\sigma_{8} = 0.8$, $n_{s} = 0.97$, $\\sum m_{\\nu} = 0.06 \\; \\mathrm{eV}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd49e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiducial cosmological parameters are:\n",
    "# dict(Omega_m=0.31, omega_b=0.022, h=0.676, sigma8=0.8, n_s=0.97, m_ncdm=0.06)\n",
    "# To compute comoving_radial_distance(z), various options:\n",
    "# 1) code this yourself if you have never done so! You just need \\Omega_{m} (flat Universe, negligible radiation: \\Omega_{\\Lambda} = 1 - \\Omega_{m})\n",
    "# H(z) = H_{0} \\sqrt{\\Omega_{m} (1 + z)^3 + \\Omega_{\\Lambda}}\n",
    "# d(z) = \\int cdz / H(z)\n",
    "# WARNING: let's work in Mpc/h units (H0 = 100 h km/s/Mpc)\n",
    "# 2) use classy, camb, astropy...\n",
    "# 3) use cosmoprimo (engine='class' or 'camb' or 'astropy'), see cell 9 of https://github.com/cosmodesi/cosmoprimo/blob/main/nb/examples.ipynb\n",
    "from cosmoprimo import Cosmology\n",
    "cosmo_fid = Cosmology(Omega_m=0.31, omega_b=0.022, h=0.676, sigma8=0.8, n_s=0.97, m_ncdm=[0.06], engine='class')\n",
    "# Or simply:\n",
    "#from cosmoprimo.fiducial import BOSS\n",
    "#cosmo_fid = BOSS(engine='class')\n",
    "\n",
    "def get_xyz(ra, dec, z):\n",
    "    # Compute distance d\n",
    "    d = cosmo_fid.comoving_radial_distance(z)\n",
    "    # Turn distance d, RA (\\phi), Dec (\\pi/2-\\theta) (mind degree -> radians!) into x, y, z Cartesian positions\n",
    "    return x, y, z\n",
    "\n",
    "#data_positions = get_xyz(data['RA'], data['DEC'], data['Z'])\n",
    "# Same for randoms\n",
    "#randoms_positions = get_xyz(randoms['RA'], randoms['DEC'], randoms['Z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f585530",
   "metadata": {},
   "source": [
    "For fun, make a 'wedge plot' of the data: a Cartesian 2D (x, y) slice between 0 and 1 deg in Dec. Do you see structures, filaments, voids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a649f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_dec = (0., 1.)\n",
    "mask_data = (data['DEC'] > lim_dec[0]) & (data['DEC'] < lim_dec[1])\n",
    "mask_randoms = (randoms['DEC'] > lim_dec[0]) & (randoms['DEC'] < lim_dec[1])\n",
    "\n",
    "plt.gcf().set_size_inches((15,) * 2)\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal')\n",
    "# Plot (-x, y) (minus sign just to orient the figure)\n",
    "# e.g. ax.scatter(- data_positions[0][mask_data], data_positions[1][mask_data], marker='.', s=4., alpha=0.8)\n",
    "#ax.tick_params(bottom=False, labelbottom=False, left=False, labelleft=False) \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166eaf32",
   "metadata": {},
   "source": [
    "In addition to completeness weights above, when computing 2pt statistics (correlation function or power spectrum), one can apply weights to minimize its variance: WEIGHT_FKP = 1/(1 + NZ * P0), with P0 the typical value of the power spectrum at the scales of interest, e.g. $10000 \\; (\\mathrm{Mpc}/h)^{3}$ (NZ is in $(\\mathrm{Mpc}/h)^{3}$).\n",
    "See e.g. https://arxiv.org/pdf/astro-ph/9304022.pdf, eq. 2.3, for the variational demonstration (another, broader point-of-view is that of the optimal quadratic estimator, of which the FKP estimator we will use below is a simplification under some assumptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e257e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute data weights, WEIGHT_COMP * WEIGHT_FKP\n",
    "data_weights =\n",
    "# Same for randoms\n",
    "randoms_weights ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c8b2f5",
   "metadata": {},
   "source": [
    "## Correlation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d76792",
   "metadata": {},
   "source": [
    "### Pair counts as a function of $(s, \\mu)$\n",
    "\n",
    "Correlation functions are usually estimated by binning the (weighted) number of pairs of particles (galaxies, randoms) as a function of the separation $s$ between particles (and optionally the cosine angle to the line-of-sight $\\mu$).\n",
    "\n",
    "This is basically a double loop, schematically:\n",
    "```\n",
    "for i1 in n1:  \n",
    "    for i2 in n2:  \n",
    "        counts[index(s(i1, i2)), index(mu(i1, i2))] += weight(i1, i2)  \n",
    "```\n",
    "For n1 ~ n2 ~ O(1e6) objects, this will be slow (especially in Python). How to speed this up?\n",
    "Auto pair counts (particles i1 and i2 come from the same catalog) can be computed ~ twice faster than cross pair counts (particles i1 and i2 come from different catalogs), why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a898a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute data - data pair counts DD\n",
    "# You can use pycorr (wrapping a modified version of Corrfunc)\n",
    "# If so, take a quick look at https://github.com/cosmodesi/pycorr/blob/main/nb/basic_examples.ipynb\n",
    "#edges = (np.linspace(0., 200, 51), np.linspace(-1., 1., 100))  # s, mu binning\n",
    "#nthreads, gpu = 4, False\n",
    "# When running on Google Colab, you can use GPU:\n",
    "#nthreads, gpu = 1, True\n",
    "#from pycorr import TwoPointCounter\n",
    "# If positions are x, y, z, pass position_type='xyz' instead\n",
    "#D1D2 = TwoPointCounter(mode='smu', edges=edges, positions1=data_positions, weights1=data_weights,\n",
    "#                       position_type='xyz', dtype='f8', engine='corrfunc', nthreads=nthreads, gpu=gpu)\n",
    "# D1D2.sep are (average) separations\n",
    "# D1D2.wcounts are pair counts (weighted by the product of particle weights)\n",
    "# D1D2.wnorm is the normalization (see below)\n",
    "# Same for randoms - randoms pair counts RR\n",
    "# WARNING: This will take a few minutes, be patient!\n",
    "#R1R2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59a910",
   "metadata": {},
   "source": [
    "Can you guess how RR counts typically evolve as a function of $s$ at small separation? This relation is not perfectly verified at small scales, due to fine-grained veto masks, but why is it really not valid anymore at large scales?\n",
    "\n",
    "You see now why randoms are important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1eabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RR(s) / f(s) counts as a function of s (log-scale)\n",
    "# (let's sum wcounts over mu to keep only the s-dependence)\n",
    "# s, wcounts, sedges = np.mean(R1R2.sep, axis=-1), np.sum(R1R2.wcounts, axis=-1), R1R2.edges[0]\n",
    "#plt.plot(s, ...)\n",
    "#plt.xscale('log')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3a342",
   "metadata": {},
   "source": [
    "### Natural estimator\n",
    "The natural estimator (the simplest one can typically think of) for the correlation function compares the (weighted) number of galaxy pairs to that of randoms (what we would have in absence of clustering), i.e. DD / RR - 1, with:\n",
    "- DD the (normalized) galaxy - galaxy pair counts\n",
    "- RR the (normalized) randoms - randoms pair counts\n",
    "\n",
    "Why 'normalized'? What is the normalization factor?\n",
    "Clue: what is the total (weighted) number of pairs (until infinite separation)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9486b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DD / RR - 1\n",
    "# WARNING: you need to normalize DD and RR first before computing DD / RR - 1!\n",
    "#D1D2_wnorm = ...\n",
    "#R1R2_wnorm = ...\n",
    "#xi = (D1D2.wcounts / D1D2_wnorm) / (R1R2.wcounts / R1R2_wnorm) - 1\n",
    "# Compute sep, mean s\n",
    "#sep = np.sum(R1R2.sep * R1R2.wcounts, axis=-1) / np.sum(R1R2.wcounts, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3aff69",
   "metadata": {},
   "source": [
    "### Multipoles of the correlation function\n",
    "Compute and plot the correlation function multipoles $\\ell = 0$ (monopole), $\\ell = 2$ (quadrupole) and $\\ell = 4$ (hexadecapole), given by $\\xi_{\\ell}(s) = \\frac{2 \\ell + 1}{2} \\int_{-1}^{1} d\\mu \\xi(s,\\mu) \\mathcal{L}_{\\ell}(\\mu)$,\n",
    "with $\\mathcal{L}_{\\ell}(\\mu)$ Legendre polynomials (see https://en.wikipedia.org/wiki/Legendre_polynomials, also https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.legendre.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick: write d\\mu \\mathcal{L}_{\\ell}(\\mu) as the difference of \\mathcal{L}_{\\ell}(\\mu) primitives\n",
    "# This makes sure $\\xi_{\\ell > 0} is 0 in case $\\xi(s, \\mu)$ is constant in \\mu\n",
    "ells = (0, 2, 4)\n",
    "# Compute xiell, list of multipoles\n",
    "#from scipy import special\n",
    "#xiell = []\n",
    "#for ell in ells:\n",
    "#    muedges = R1R2.edges[1]\n",
    "#    poly = special.legendre(ell).integ()(muedges)\n",
    "#    legendre = (2 * ell + 1) * (poly[1:] - poly[:-1])\n",
    "#    xiell.append(...)\n",
    "\n",
    "# Plot the correlation function multipoles\n",
    "#for ill, ell in enumerate(ells):\n",
    "#    plt.plot(sep, sep**2 * xiell[ill], label='$\\ell = {:d}$'.format(ell))\n",
    "#plt.grid(True)\n",
    "#plt.xlabel(r'$s$ [$\\mathrm{Mpc}/h$]')\n",
    "#plt.ylabel(r'$s^{2}\\xi_{\\ell}(s)$ [$(\\mathrm{Mpc}/h)^{2}$]')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da18beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes about pycorr\n",
    "# You can obtain this directly with:\n",
    "#from pycorr import NaturalTwoPointEstimator\n",
    "#estimator = NaturalTwoPointEstimator(D1D2=D1D2, R1R2=R1R2)\n",
    "#s, xiell = estimator(ells=ells, return_sep=True)\n",
    "#estimator.plot(ells=ells)\n",
    "# Or, starting from the beginning:\n",
    "#from pycorr import TwoPointCorrelationFunction, setup_logging\n",
    "#setup_logging()  # to activate logging\n",
    "#estimator = TwoPointCorrelationFunction(mode='smu', edges=edges, data_positions1=data_positions, data_weights1=data_weights,\n",
    "#                                        randoms_positions1=randoms_positions, randoms_weights1=randoms_weights,\n",
    "#                                        estimator='natural', position_type='xyz', dtype='f8', engine='corrfunc', nthreads=nthreads, gpu=gpu)\n",
    "#estimator.plot(ells=ells)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc33a4",
   "metadata": {},
   "source": [
    "The monopole looks odd (does not go to ~ zero) on large scales!\n",
    "\n",
    "No worries, this is not a bias (in average, it would be fine), but the natural estimator has a larger variance than the Landy-Szalay estimator (see e.g. https://articles.adsabs.harvard.edu/pdf/1993ApJ...412...64L): (DD - 2DR + RR) / RR.\n",
    "\n",
    "Estimate the correlation function with the Landy-Szalay estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can compute all pair counts DD, DR and RR as above\n",
    "# Or use pycorr's general interface (see https://github.com/cosmodesi/pycorr/blob/main/nb/basic_examples.ipynb)\n",
    "#from pycorr import TwoPointCorrelationFunction\n",
    "#estimator = TwoPointCorrelationFunction(mode='smu', edges=edges, ..., estimator='landyszalay')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd42806",
   "metadata": {},
   "source": [
    "Plot the correlation function multipoles. Looks better!  \n",
    "You should clearly see the BAO peak (where?).\n",
    "What does the non-zero quadrupole mean? Where does it come from?\n",
    "\n",
    "You can also compare data to a theory correlation function (just scale the linear correlation function by an arbitrary factor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "#estimator.plot(ells=ells)\n",
    "#zeff = np.average(data['Z'], weights=data_weights)  # (quite arbitrary) effective redshift\n",
    "\n",
    "# Linear (real-space) correlation function:\n",
    "# xi = cosmo_fid.get_fourier().pk_interpolator().to_1d(z=zeff).to_xi()(sep)\n",
    "#kaiser_factor = 6.5  # scaling by hand, just to match the amplitude\n",
    "#xi_model = kaiser_factor * cosmo_fid.get_fourier().pk_interpolator().to_1d(z=zeff).to_xi()(sep)\n",
    "#plt.plot(sep, sep**2 * xi_model, color='k')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12371e26",
   "metadata": {},
   "source": [
    "## Power spectrum\n",
    "\n",
    "Computing pair counts for correlation function estimation remains somewhat slow (still tractable for current surveys, DESI, Euclid). Also, theorists tend to prefer the power spectrum, as different $k$-modes are initially (almost) uncorrelated and evolve independently in the linear regime (see Julien's course).\n",
    "Let's compute the power spectrum monopole step-by-step, to show how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da3fd7",
   "metadata": {},
   "source": [
    "First, \"paint\" data and randoms to a 3D mesh, to get an estimate of the 3D over density field $F(\\mathbf{r}) = n_{d}(\\mathbf{r}) - \\alpha n_{r}(\\mathbf{r})$, with:\n",
    "- $n_{d}(\\mathbf{r})$ the data density\n",
    "- $n_{r}(\\mathbf{r})$ the randoms density\n",
    "- $\\alpha = \\sum_{i \\in \\mathrm{data}} w_{i} / \\sum_{i \\in \\mathrm{randoms}} w_{i}$: scale the randoms density to the data density, i.e. such that $\\int d^3 r F(\\mathbf{r}) = 0$ (integral constraint!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c393454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypower import CatalogMesh\n",
    "# This step involves a kernel (resampler='tsc') that effectively smoothes the density field,\n",
    "# which must be compensated for by a kernel in Fourier space (compensate=True),\n",
    "# see e.g. https://arxiv.org/abs/astro-ph/0409240\n",
    "# Wider is this kernel, better is the mitigation of aliasing effects (greater than Nyquist frequencies contaminating lower frequencies)\n",
    "# Another technique to mitigate aliasing is 'interlacing': shifting the mesh by a fraction of mesh cell size\n",
    "# see e.g. https://arxiv.org/abs/1512.07295\n",
    "# WARNING: If you have < 8 GB RAM, use smaller nmesh (and / or dtype='f4' i.e. simple precision)\n",
    "#mesh = CatalogMesh(data_positions=data_positions, data_weights=data_weights,\n",
    "#                   randoms_positions=randoms_positions, randoms_weights=randoms_weights,\n",
    "#                   boxsize=5000., nmesh=256, resampler='tsc', interlacing=3, position_type='xyz',\n",
    "#                   dtype='f8', mpiroot=0)\n",
    "#rfield = mesh.to_mesh(compensate=True)\n",
    "# To estimate the density, divide by the cell volume:\n",
    "#cellsize = mesh.boxsize / mesh.nmesh\n",
    "#dv = cellsize.prod()\n",
    "#rfield /= dv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db732cf",
   "metadata": {},
   "source": [
    "Next, the power spectrum is (up to some normalization A), the square modulus of the Fourier-space field:\n",
    "$\\hat{P}(\\mathbf{k}) = |F(\\mathbf{k})|^2 / A - S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute F(k)\n",
    "#cfield = rfield.r2c()  # Fast Fourier Transform, F(r) -> F(k)\n",
    "# FFT convention is F(k) = 1/N^3 \\sum_{r} e^{-ikr} F(r), so compensate by N^3 * dv\n",
    "#cfield *= mesh.nmesh.prod() * dv\n",
    "#cfield[...] = cfield[...] * cfield[...].conj()  # |F(k)|^2\n",
    "#del rfield  # save memory\n",
    "#print(cfield.value[0, 0, 0])  0 by construction: integral constraint!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fef8a8",
   "metadata": {},
   "source": [
    "We want to compute the 1D power spectrum, as a function of the norm $k = |\\mathbf{k}|$. Let's bin $|F(\\mathbf{k})|^2$ as a function of  $k = |\\mathbf{k}|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fe8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges = (k-edges, mu-edges = [-1, 1])\n",
    "#knyq = np.pi / np.max(cellsize)  # Nyquist frequency\n",
    "#edges = (np.arange(0., knyq, 0.005), np.linspace(-1., 1., 2))\n",
    "#from pypower.fft_power import project_to_basis\n",
    "#k, _, pk, nk, _ = project_to_basis(cfield, edges)[0]\n",
    "#k, pk = k.ravel(), pk.ravel().real\n",
    "#del cfield  # save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02db8e",
   "metadata": {},
   "source": [
    "Let's find the normalization $\\int d^3 r \\bar{F}^2(\\mathbf{r})$, with $\\bar{F}$ the selection function (sampled by the randoms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e618bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick: estimate \\bar{F}^2 with data x randoms instead of randoms^2 to avoid shot noise\n",
    "#wnorm = (mesh.to_mesh(field='data', compensate=False) / dv * mesh.to_mesh(field='data-normalized_randoms', compensate=False) / dv).csum() * dv\n",
    "#pk /= wnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb45a2d",
   "metadata": {},
   "source": [
    "Plot the power spectrum (typically $k P(k)$). What is its unit?\n",
    "The small scales (high $k$) monopole looks fishy (too high --- it should ~ decrease), did we forget anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2b6ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "#plt.plot(k, k * pk)\n",
    "#plt.xlabel(...)\n",
    "#plt.ylabel(...)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a67484",
   "metadata": {},
   "source": [
    "Indeed, we forgot to remove the Poisson shot noise $S$! What is it?\n",
    "\n",
    "$S$ is to be computed as $(\\sum_{i \\in \\mathrm{data}} w_{i}^{2} + \\alpha^2 \\sum_{i \\in \\mathrm{randoms}} w_{i}^{2}) / A$.\n",
    "Subtract it from the estimated power spectrum, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c1c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd9367",
   "metadata": {},
   "source": [
    "Looks much better! Do you recognize the BAO wiggles? To see them better, you can compare data to a theory power spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b90f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "#pk_model = kaiser_factor * cosmo_fid.get_fourier().pk_interpolator().to_1d(z=zeff)(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae03bc",
   "metadata": {},
   "source": [
    "### Multipoles of the power spectrum\n",
    "Estimating higher multipoles (quadrupole, hexadecapole) is slightly more difficult, as we have to account for varying line-of-sight. We typically use the first-point / end-point for line-of-sight $\\hat{\\mathbf{\\eta}}$, to split:\n",
    "$\\hat{P}(\\mathbf{k}) = (2\\ell + 1) / A \\sum_{\\mathbf{r}_{1}, \\mathbf{r}_{2}} e^{ik(\\mathbf{r}_{2} - \\mathbf{r}_{1})} F(\\mathbf{r}_{1}) F(\\mathbf{r}_{2}) \\mathcal{L}_{\\ell}(\\hat{\\mathbf{k}} \\cdot \\hat{\\mathbf{\\eta}}) - \\delta_{\\ell 0} S$\n",
    "into:\n",
    "$\\hat{P}(\\mathbf{k}) = (2\\ell + 1) / A F_{\\ell}^{\\star}(\\mathbf{k}) F_{0}(\\mathbf{k}) - \\delta_{\\ell 0} S$\n",
    "with:\n",
    "$F_{\\ell}(\\mathbf{k}) = \\sum_{\\mathbf{r}} e^{i\\mathbf{k} \\cdot \\mathbf{r}} F(r) \\mathcal{L}_{\\ell}(\\hat{\\mathbf{k}} \\cdot \\hat{\\mathbf{r}})$\n",
    "This last term can be split, using e.g. decomposition of Legendre polynomial into spherical harmonics, into a pure $\\mathbf{k}$-dependent term, times a standard Fourier transform, which can be FFT'ed. See e.g. https://arxiv.org/abs/1704.02357."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this, use pypower.\n",
    "# (You can take a quick look at https://github.com/cosmodesi/pypower/blob/main/nb/basic_examples.ipynb)\n",
    "#from pypower import CatalogFFTPower\n",
    "#power = CatalogFFTPower(data_positions1=data_positions, data_weights1=data_weights,\n",
    "#                        randoms_positions1=randoms_positions, randoms_weights1=randoms_weights,\n",
    "#                        boxsize=5000., nmesh=256, resampler='tsc', interlacing=3,\n",
    "#                        edges={'step': 0.005}, ells=(0, 2, 4), position_type='xyz', dtype='f8', mpiroot=0).poles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f2a0b9",
   "metadata": {},
   "source": [
    "Plot the power spectrum multipoles.\n",
    "Again, BAO wiggles and non-zero quadrupole!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e45df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "#power.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45265bc",
   "metadata": {},
   "source": [
    "Note that the above power spectrum measurements (using the 'FKP estimator') cannot (should not) be directly compared to theory power spectrum models... Indeed, in these measurements the power spectrum is convolved with the survey selection function (that we essentially already removed in the correlation function estimation with the division by RR). Therefore, usually people multiply the theory power spectrum by the window matrix, which can be computed from the random catalogs; see https://github.com/cosmodesi/pypower/blob/main/nb/window_examples.ipynb if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b76a2",
   "metadata": {},
   "source": [
    "### Kaiser formula\n",
    "Still, let's just look how the power spectrum multipoles compare to the Kaiser formula, which gives the expected power spectrum at linear order (i.e. accurate on large scales = low $k$):\n",
    "$P(k, \\mu) = (b + f \\mu^2)^{2} P^{\\mathrm{lin}}(k) = (\\beta^{-1} + \\mu^2)^{2} P^{\\mathrm{lin}}_{\\theta\\theta}(k)$ ($\\beta = f / b$)\n",
    "where $b$ is the galaxy bias, $f = \\frac{d\\ln{D}}{d\\ln{a}} \\simeq \\Omega_m(z)^{0.55}$ is the logarithmic growth rate of structure, $P^{\\mathrm{lin}}(k)$ is the linear power spectrum.\n",
    "\n",
    "The model depends on $f (\\times \\sigma_{8})$ or --- better formulated --- the amplitude of the velocity divergence power spectrum $P_{\\theta\\theta}(k)$, which we can fit to the data.\n",
    "\n",
    "Integrate $P(k, \\mu)$ over Legendre polynomials, either symbolically (easy!) or by numerical integration, and plot model and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72945f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pklin = cosmo_fid.get_fourier().pk_interpolator()(k, z=zeff)\n",
    "#f = cosmo_fid.growth_rate(zeff)\n",
    "#b =  # tune by hand to match data amplitude\n",
    "pk_model = []\n",
    "# Append the multipoles\n",
    "#power.plot()\n",
    "#for ill, ell in enumerate(ells):\n",
    "#    plt.plot(k, k * pk_model[ill], color='C{:d}'.format(ill), linestyle='--')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e12a7",
   "metadata": {},
   "source": [
    "The quadrupole in particular looks a small scales (high $k$) a bit more damped in the data than in the model, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397adbe",
   "metadata": {},
   "source": [
    "## Take-home messages\n",
    "- in standard analyses, the observed (RA, Dec, z) are compressed into power spectrum or correlation function multipoles\n",
    "- BAO: peak at 100 $\\mathrm{Mpc}/h$ in the correlation function, wiggles in the power spectrum\n",
    "- measuring the position of the BAO peak = measuring a fixed comoving distance ('standard ruler') at a given redshift = constraing the Universe's expansion\n",
    "- non-zero quadrupole = anisotropy (w.r.t the line-of-sight): redshift-space distortions (linear model: Kaiser formula)\n",
    "- measuring redshift-space distortions = constraining the amplitude of the velocity power spectrum: growth of structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5973ea6",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "- pen-and-paper exercise: derive the Kaiser formula, knowing that:\n",
    "$\\mathbf{s} = \\mathbf{r} + v_{z} \\hat{\\mathbf{z}}$ with $v_{z}(\\mathbf{k}) = i f \\mathbf{k} / k^{2} \\delta_{r}(\\mathbf{k})$ (which can be obtained from linear theory)\n",
    "and mass conservation (mass in redshift space = mass in real space): $\\left[1 + \\delta_{s}(\\mathbf{s})\\right]d^{3}s = \\left[1 + \\delta_{r}(\\mathbf{r})\\right]d^{3}r$. Correction: Section II. of https://arxiv.org/abs/1006.0699.\n",
    "- we have worked so far with 'pre-reconstruction' catalogs. A step called 'reconstruction' can be used to sharpen the BAO peak. Compute the correlation function and / or power spectrum of reconstructed catalogs (with *rec* in name).\n",
    "Note that, in this case, you should provide reconstructed data for data_positions and data_weights, reconstructed randoms for shifted_positions and shifted_weights, and pre-reconstruction randoms for randoms_positions, randons_weights. What is the difference w.r.t. pre-reconstruction measurements? (BAO peak, quadrupole?).\n",
    "- you can try applying reconstruction yourself to pre-reconstruction catalogs with https://github.com/cosmodesi/pyrecon\n",
    "- here pair weights are simple product of galaxy individual weights... but we can imagine other schemes, e.g. to correctly compensate for fiber collisions: see https://github.com/cosmodesi/pycorr/blob/main/nb/pip_examples.ipynb (and similar correction for power spectrum, https://github.com/cosmodesi/pypower/blob/main/nb/pip_examples.ipynb)\n",
    "- start Y2 TD, cosmological constraints with BAO: bao_inverse_distance_ladder.ipynb\n",
    "- why are you still here? go to the beach..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmodesi-main",
   "language": "python",
   "name": "cosmodesi-main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
